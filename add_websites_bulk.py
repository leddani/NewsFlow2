#!/usr/bin/env python3
"""
Script pÃ«r shtimin masiv tÃ« website-ve shqiptare nÃ« NewsFlow AI Editor.
"""

import requests
import json
import time

BASE_URL = "http://localhost:8000"

# Lista e website-ve shqiptare pÃ«r lajme
WEBSITES = [
    {
        "name": "Gazeta Express",
        "url": "https://www.gazetaexpress.com/",
        "scrape_interval_minutes": 7,
        "active": True
    },
    {
        "name": "RTSH News",
        "url": "https://www.rtsh.al/",
        "scrape_interval_minutes": 8,
        "active": True
    },
    {
        "name": "News24 Albania",
        "url": "https://news24.al/",
        "scrape_interval_minutes": 4,
        "active": True
    },
    {
        "name": "Koha Jone",
        "url": "https://kohajone.com/",
        "scrape_interval_minutes": 6,
        "active": True
    },
    {
        "name": "realitetipost",
        "url": "https://www.realitetipost.net/",
        "scrape_interval_minutes": 9,
        "active": True
    },
    {
        "name": "shkodrazone",
        "url": "https://shkodrazone.com/",
        "scrape_interval_minutes": 10,
        "active": True
    },
    {
        "name": "Balkan Web",
        "url": "https://www.balkanweb.com/",
        "scrape_interval_minutes": 5,
        "active": True
    },
    {
        "name": "Shekulli Online",
        "url": "https://shekulli.com.al/",
        "scrape_interval_minutes": 8,
        "active": True
    },
    {
        "name": "Reporter.al",
        "url": "https://www.reporter.al/",
        "scrape_interval_minutes": 6,
        "active": True
    },
    {
        "name": "Gazeta Panorama",
        "url": "https://www.panorama.com.al/",
        "scrape_interval_minutes": 7,
        "active": True
    }
]

def add_websites():
    """Add all websites to the system."""
    print("ðŸŒ SHTIMI I WEBSITE-VE SHQIPTARE")
    print("=" * 50)
    
    added_count = 0
    failed_count = 0
    
    for i, website in enumerate(WEBSITES, 1):
        print(f"\n{i}. ðŸ“° {website['name']}")
        print(f"   ðŸ”— {website['url']}")
        print(f"   â±ï¸  Interval: {website['scrape_interval_minutes']} min")
        
        try:
            response = requests.post(
                f"{BASE_URL}/websites",
                json=website,
                timeout=10
            )
            
            if response.status_code == 200:
                result = response.json()
                print(f"   âœ… Shtuar me sukses! (ID: {result.get('id', 'N/A')})")
                added_count += 1
            else:
                print(f"   âŒ Gabim: {response.status_code} - {response.text}")
                failed_count += 1
                
        except Exception as e:
            print(f"   âŒ Gabim nÃ« connection: {str(e)}")
            failed_count += 1
            
        # Small delay between requests
        time.sleep(0.5)
    
    print(f"\nðŸ“Š PÃ‹RMBLEDHJE:")
    print(f"âœ… Website tÃ« shtuar: {added_count}")
    print(f"âŒ DÃ«shtime: {failed_count}")
    print(f"ðŸ“ Total: {len(WEBSITES)}")
    
    if added_count > 0:
        print(f"\nðŸŽ¯ HAPE SWAGGER UI: http://localhost:8000/docs")
        print(f"ðŸ“‹ Kontroll website-t: GET /websites")
        print(f"ðŸš€ Start scheduler: POST /scheduler/start")

if __name__ == "__main__":
    add_websites() 