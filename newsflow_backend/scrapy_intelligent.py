#!/usr/bin/env python3
"""
Scrapy Engine i InteligjentÃ« pÃ«r NewsFlow AI Editor

Engine i ri qÃ« e kupton se cilat janÃ« lajmet mÃ« tÃ« reja:
1. Gjen tÃ« gjitha lajmet nga homepage
2. I organizon sipas pozicionit (lajmet e para janÃ« mÃ« tÃ« reja)
3. Kontrollon nÃ« databazÃ« se cilat i ka parÃ« tashmÃ«
4. Ndalon kur gjen lajme tÃ« njohura
5. Kthen vetÃ«m lajmet e reja
"""

import json
import logging
import requests
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import re
from datetime import datetime
from sqlalchemy.orm import Session
from .database import SessionLocal
from . import crud

logger = logging.getLogger(__name__)

class IntelligentNewsScraper:
    """Scrapy Engine i inteligjentÃ« qÃ« e kupton lajmet mÃ« tÃ« reja"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'NewsFlow-AI-Editor (+https://newsflow.ai)',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'sq,en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
    
    def scrape_intelligent(self, base_url: str, max_articles: int = 10) -> List[Dict[str, Any]]:
        """
        Scrape intelligent qÃ« gjen vetÃ«m lajmet e reja
        
        Args:
            base_url: URL-ja e faqes kryesore (p.sh. https://www.balkanweb.com/)
            max_articles: Numri maksimal i artikujve pÃ«r t'u kontrolluar
            
        Returns:
            Lista e lajmeve tÃ« reja
        """
        try:
            logger.info(f"ğŸ§  Scrapy Intelligent: Duke filluar pÃ«r {base_url}")
            
            # Hapi 1: Merr homepage-in
            response = requests.get(base_url, headers=self.headers, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Hapi 2: Gjen tÃ« gjitha linqet e lajmeve
            news_links = self._extract_news_links(soup, base_url)
            logger.info(f"ğŸ” Gjeta {len(news_links)} linqe potenciale lajmesh")
            
            if not news_links:
                logger.warning(f"âš ï¸ Nuk gjeta linqe lajmesh nÃ« {base_url}")
                return []
            
            # Hapi 3: Kontrollo dhe scrape vetÃ«m lajmet e reja
            new_articles = []
            checked_count = 0
            
            for i, link in enumerate(news_links[:max_articles]):
                checked_count += 1
                
                # Kontrollo nÃ«se ky artikull ekziston nÃ« databazÃ«
                if self._article_exists_in_database(link):
                    logger.info(f"ğŸ“š Artikulli {i+1} ekziston nÃ« DB: {link[:60]}...")
                    logger.info(f"ğŸ›‘ NDALOJ kÃ«tu - kam arritur lajme tÃ« njohura (pozicioni {i+1})")
                    break
                
                # Artikull i ri - scrape-o
                logger.info(f"ğŸ†• Artikull i ri {i+1}: {link[:60]}...")
                article_data = self._scrape_single_article(link)
                
                if article_data:
                    new_articles.append(article_data)
                    logger.info(f"âœ… Scrape u krye pÃ«r: {article_data['title'][:50]}...")
            
            logger.info(f"ğŸ¯ Scrapy Intelligent REZULTATI:")
            logger.info(f"   ğŸ“Š Artikuj tÃ« kontrolluar: {checked_count}")
            logger.info(f"   ğŸ†• Artikuj tÃ« rinj: {len(new_articles)}")
            
            return new_articles
            
        except Exception as e:
            logger.error(f"âŒ Scrapy Intelligent error pÃ«r {base_url}: {e}")
            return []
    
    def _extract_news_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """Ekstrakton tÃ« gjitha linqet e lajmeve nga homepage, tÃ« organizuara sipas pozicionit"""
        
        # SelektorÃ« tÃ« ndryshÃ«m pÃ«r lajme (tÃ« organizuar sipas prioritetit)
        news_selectors = [
            # SelektorÃ« tÃ« pÃ«rbashkÃ«t pÃ«r lajme
            'article a[href]',
            '.article a[href]',
            '.news a[href]',
            '.post a[href]',
            '.entry a[href]',
            
            # SelektorÃ« pÃ«r tituj
            'h1 a[href]',
            'h2 a[href]',
            'h3 a[href]',
            
            # SelektorÃ« tÃ« specializuar
            '.latest-news a[href]',
            '.news-item a[href]',
            '.story a[href]',
            '.headline a[href]',
            
            # SelektorÃ« tÃ« pÃ«rgjithshÃ«m (si fallback)
            'a[href*="/article/"]',
            'a[href*="/news/"]',
            'a[href*="/lajme/"]',
            'a[href*="/aktualitet/"]',
        ]
        
        found_links = []
        domain = urlparse(base_url).netloc
        
        for selector in news_selectors:
            try:
                links = soup.select(selector)
                for link in links:
                    href = link.get('href')
                    if href and self._is_valid_news_link(href, domain):
                        full_url = urljoin(base_url, href)
                        if full_url not in found_links:
                            found_links.append(full_url)
            except Exception as e:
                logger.debug(f"Selector error {selector}: {e}")
        
        # Limitoj dhe kthej linqet (tÃ« organizuara sipas pozicionit nÃ« homepage)
        return found_links[:20]  # Maksimum 20 linqe pÃ«r performancÃ«
    
    def _is_valid_news_link(self, href: str, domain: str) -> bool:
        """Kontrollon nÃ«se njÃ« link Ã«shtÃ« link i vlefshÃ«m lajmi"""
        
        if not href or href.startswith('#') or href.startswith('javascript:'):
            return False
        
        # PÃ«rjashto llojet e tjera tÃ« faqeve
        exclude_patterns = [
            '/category/', '/tag/', '/author/', '/search/',
            '/contact/', '/about/', '/privacy/', '/terms/',
            '.pdf', '.jpg', '.png', '.gif', '.mp4', '.mp3',
            'mailto:', 'tel:', 'facebook.com', 'twitter.com',
            'instagram.com', 'youtube.com', '#comment'
        ]
        
        for pattern in exclude_patterns:
            if pattern in href.lower():
                return False
        
        # Prefero linqet e brendshÃ«m
        if href.startswith('http'):
            return domain in href
        
        return True
    
    def _article_exists_in_database(self, url: str) -> bool:
        """Kontrollon nÃ«se njÃ« artikull ekziston nÃ« databazÃ«"""
        try:
            db = SessionLocal()
            try:
                existing_article = crud.get_article_by_url(db, url)
                return existing_article is not None
            finally:
                db.close()
        except Exception as e:
            logger.debug(f"Database check error pÃ«r {url}: {e}")
            return False
    
    def _scrape_single_article(self, url: str) -> Optional[Dict[str, Any]]:
        """Scrape njÃ« artikull tÃ« vetÃ«m"""
        try:
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Ekstrakto title
            title = self._extract_title(soup)
            
            # Ekstrakto content
            content = self._extract_content(soup)
            
            # Ekstrakto images dhe videos
            images = self._extract_images(soup, url)
            videos = self._extract_videos(soup)
            
            if not title or not content:
                logger.warning(f"âš ï¸ Title ose content bosh pÃ«r {url}")
                return None
            
            return {
                'title': title,
                'url': url,
                'content': content,
                'images': images,
                'videos': videos
            }
            
        except Exception as e:
            logger.error(f"âŒ Error scraping single article {url}: {e}")
            return None
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """Ekstrakton titullin e artikullit"""
        
        # SelektorÃ« tÃ« ndryshÃ«m pÃ«r title
        title_selectors = [
            'h1',
            '.article-title',
            '.post-title', 
            '.entry-title',
            'title',
            '[property="og:title"]',
            '[name="title"]'
        ]
        
        for selector in title_selectors:
            try:
                element = soup.select_one(selector)
                if element:
                    title = element.get_text().strip() if element.name != 'meta' else element.get('content', '')
                    if title and len(title) > 10:
                        # Pastro nga emrat e website-it
                        title = self._clean_title(title)
                        return title
            except Exception:
                continue
        
        return "Titull i panjohur"
    
    def _clean_title(self, title: str) -> str:
        """Pastron titullin nga emrat e website-it"""
        
        # Hiq separatorÃ«t dhe emrat e website-it
        for separator in [' â€“ ', ' - ', ' | ', ' :: ', ' â€” ']:
            if separator in title:
                parts = title.split(separator)
                title = parts[0].strip()  # Merr vetÃ«m pjesÃ«n e parÃ«
                break
        
        # Hiq pattern-et e zakonshme
        patterns_to_remove = [
            r'^(Kreu|Home|News|Lajme)\s*[-â€“â€”]\s*',
            r'\s*[-â€“â€”]\s*\w+\.(com|al|net|org).*$',
            r'\s*\|\s*\w+\.(com|al|net|org).*$'
        ]
        
        for pattern in patterns_to_remove:
            title = re.sub(pattern, '', title, flags=re.IGNORECASE).strip()
        
        return title
    
    def _extract_content(self, soup: BeautifulSoup) -> str:
        """Ekstrakton pÃ«rmbajtjen e artikullit"""
        
        # SelektorÃ« pÃ«r content
        content_selectors = [
            '.article-content',
            '.post-content',
            '.entry-content', 
            '.content',
            'article',
            '.story-body',
            '.news-content'
        ]
        
        content_parts = []
        
        for selector in content_selectors:
            try:
                container = soup.select_one(selector)
                if container:
                    # Merr tÃ« gjitha paragrafÃ«t
                    paragraphs = container.find_all('p')
                    for p in paragraphs:
                        text = p.get_text().strip()
                        if text and len(text) > 20:
                            content_parts.append(text)
                    
                    if content_parts:
                        break
            except Exception:
                continue
        
        # Fallback: merr tÃ« gjitha paragrafÃ«t nga faqja
        if not content_parts:
            paragraphs = soup.find_all('p')
            for p in paragraphs:
                text = p.get_text().strip()
                if text and len(text) > 20:
                    content_parts.append(text)
        
        content = ' '.join(content_parts)
        return content if len(content) > 50 else "PÃ«rmbajtje e shkurtÃ«r"
    
    def _extract_images(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """Ekstrakton imazhet nga artikulli"""
        images = []
        
        img_tags = soup.find_all('img')
        for img in img_tags:
            src = img.get('src') or img.get('data-src')
            if src:
                full_img_url = urljoin(base_url, src)
                if self._is_valid_image_url(full_img_url):
                    images.append(full_img_url)
        
        return list(set(images))[:5]  # Maksimum 5 imazhe
    
    def _extract_videos(self, soup: BeautifulSoup) -> List[str]:
        """Ekstrakton videos nga artikulli"""
        videos = []
        
        # YouTube dhe Vimeo embeddings
        video_selectors = [
            'iframe[src*="youtube.com"]',
            'iframe[src*="youtu.be"]',
            'iframe[src*="vimeo.com"]',
            'a[href*="youtube.com"]',
            'a[href*="youtu.be"]'
        ]
        
        for selector in video_selectors:
            elements = soup.select(selector)
            for element in elements:
                url = element.get('src') or element.get('href')
                if url:
                    videos.append(url)
        
        return list(set(videos))[:3]  # Maksimum 3 video
    
    def _is_valid_image_url(self, url: str) -> bool:
        """Kontrollon nÃ«se URL-ja Ã«shtÃ« imazh i vlefshÃ«m"""
        if not url:
            return False
        
        # Kontrollo ekstensionin
        image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg']
        url_lower = url.lower()
        
        for ext in image_extensions:
            if ext in url_lower:
                return True
        
        # Kontrollo pÃ«r fjalÃ« kyÃ§e
        if any(keyword in url_lower for keyword in ['image', 'img', 'photo', 'picture']):
            return True
        
        return False


# Instanca globale
intelligent_scraper = IntelligentNewsScraper()

def scrape_with_intelligent_scrapy(url: str, max_articles: int = 10) -> List[Dict[str, Any]]:
    """
    Funksion helper pÃ«r scraping tÃ« inteligjentÃ«
    
    Args:
        url: URL-ja e faqes kryesore
        max_articles: Numri maksimal i artikujve pÃ«r t'u kontrolluar
        
    Returns:
        Lista e lajmeve tÃ« reja
    """
    return intelligent_scraper.scrape_intelligent(url, max_articles) 